{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AI Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's define our prompt and give our application access to the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize web search tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=1)\n",
    "\n",
    "# Define prompt template\n",
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Application Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic here is the same as in the tracing module. We define a search step to scan the web, and an explain step for an LLM to summarize the web results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "\n",
    "# Create Application\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "@traceable\n",
    "def search(question):\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    return web_results\n",
    "    \n",
    "@traceable\n",
    "def explain(question, context):\n",
    "    formatted = prompt.format(question=question, context=context)\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": formatted},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"o3-mini\",\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def eli5(question):\n",
    "    context = search(question)\n",
    "    answer = explain(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to run experiments, and test our application's performance on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import LangSmith Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create a LangSmith client to use the SDK, and specify the dataset we'd like to run our experiment on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Upload Dataset to LangSmith\n",
    "Before running our experiment, we need to upload our dataset to LangSmith. We'll read the CSV file and create a dataset with the name \"eli5-golden\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"eli5-golden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "\n",
    "# Read the dataset CSV file\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Convert to LangSmith format\n",
    "examples = []\n",
    "for _, row in df.iterrows():\n",
    "    examples.append({\n",
    "        \"inputs\": {\"question\": row[\"input_question\"]},\n",
    "        \"outputs\": {\"output\": row[\"output_output\"]}\n",
    "    })\n",
    "\n",
    "# Create dataset in LangSmith\n",
    "try:\n",
    "    # Try to read the dataset first to see if it already exists\n",
    "    existing_dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists with {len(list(client.list_examples(dataset_name=dataset_name)))} examples\")\n",
    "except:\n",
    "    # Dataset doesn't exist, create it\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"ELI5 (Explain Like I'm 5) dataset for evaluating AI explanations\"\n",
    "    )\n",
    "    \n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(\n",
    "        inputs=[ex[\"inputs\"] for ex in examples],\n",
    "        outputs=[ex[\"outputs\"] for ex in examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully created dataset '{dataset_name}' with {len(examples)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Code Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a custom code evaluator, which are useful to measure deterministic or close-ended metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(outputs: dict) -> bool:\n",
    "    words = outputs[\"output\"].split(\" \")\n",
    "    return len(words) <= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular custom code evaluator is a simple Python function that checks if our application produces outputs that are less than or equal to 200 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended metrics, it's can be powerful to use an LLM to score the outputs.\n",
    "\n",
    "Let's use an LLM to check whether our application produces correct outputs. First, let's define a scoring schema for our LLM to adhere to in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a scoring schema that our LLM must adhere to\n",
    "class CorrectnessScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer when compared to the reference answer.\"\"\"\n",
    "    score: int = Field(description=\"The score of the correctness of the answer, from 0 to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to give an LLM our application's outputs, alongside the reference outputs stored in our dataset. \n",
    "\n",
    "The LLM will then be able to reference the \"right\" output to judge if our application's answer meets our accuracy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    prompt = \"\"\"\n",
    "    You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
    "\n",
    "    <Rubric>\n",
    "        A correct answer:\n",
    "        - Provides accurate information\n",
    "        - Uses suitable analogies and examples\n",
    "        - Contains no factual errors\n",
    "        - Is logically consistent\n",
    "\n",
    "        When scoring, you should penalize:\n",
    "        - Factual errors\n",
    "        - Incoherent analogies and examples\n",
    "        - Logical inconsistencies\n",
    "    </Rubric>\n",
    "\n",
    "    <Instructions>\n",
    "        - Carefully read the input and output\n",
    "        - Use the reference output to determine if the model output contains errors\n",
    "        - Focus whether the model output uses accurate analogies and is logically consistent\n",
    "    </Instructions>\n",
    "\n",
    "    <Reminder>\n",
    "        The analogies in the output do not need to match the reference output exactly. Focus on logical consistency.\n",
    "    </Reminder>\n",
    "\n",
    "    <input>\n",
    "        {}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "        {}\n",
    "    </output>\n",
    "\n",
    "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "    <reference_outputs>\n",
    "        {}\n",
    "    </reference_outputs>\n",
    "    \"\"\".format(inputs[\"question\"], outputs[\"output\"], reference_outputs[\"output\"])\n",
    "    structured_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessScore)\n",
    "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    return generation.score == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Run Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to run our application on the example inputs of our dataset. This is function that will be called when we run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a function to run your application\n",
    "def run(inputs: dict):\n",
    "    return eli5(inputs[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary components, so let's run our experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "evaluate(\n",
    "    run,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, conciseness],\n",
    "    experiment_prefix=\"eli5-o3-mini\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
