{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Like I'm 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture](images/architecture.png)\n",
    "\n",
    "In this notebook, we're going to walk through setting up a simple chatbot in LangGraph. \n",
    "\n",
    "Throughout this process, we're going to show how LangSmith can be used to improve the developer experience for AI applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading our environment variables from our .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "# Loads the following env variables\n",
    "# LANGSMITH_TRACING=true\n",
    "# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGSMITH_PROJECT=\"eli5-bot\"\n",
    "# LANGSMITH_API_KEY=\"<redacted>\"\n",
    "\n",
    "# OPENAI_API_KEY=\"<redacted>\"\n",
    "# TAVILY_API_KEY=\"<redacted>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a tool called Tavily to allow our assistant to search the web when answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a prompt for RAG that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
      "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
      "You have provided with relevant background context to answer the question.\n",
      "\n",
      "Question: {question} \n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "@traceable\n",
    "def search(question):\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    return web_results\n",
    "    \n",
    "@traceable\n",
    "def explain(question, context):\n",
    "    formatted = prompt.format(question=question, context=context)\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": formatted},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def eli5(question):\n",
    "    context = search(question)\n",
    "    answer = explain(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, imagine you have a big box of LEGO blocks. Each block can do something special, like make a car or a house. Now, building something cool with LEGO can take a lot of time and you need to know how to put the pieces together just right.\n",
      "\n",
      "LangChain is like a magical instruction book that helps people use those LEGO blocks quickly and easily to make amazing things with words, like stories or answers, using computers. It has special tools that help you tell the computer exactly what you want it to say, kind of like giving it the right instructions.\n",
      "\n",
      "So, LangChain is important because it makes it easier for people to create things that use language and helps them do it faster without getting confused. Just like having a great instruction book can help you build your LEGO set perfectly!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "print(eli5(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Using LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the State for our Graph. We'll track the user's question, our application's generation, and the list of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's define the nodes of our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Web search\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "    \n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(question=question, context=\"\\n\".join([d.page_content for d in documents]))\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function to pretty print our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    for output in app.stream({\"question\": question}, stream_mode=\"updates\"):\n",
    "        if END in output or START in output:\n",
    "            continue\n",
    "        # Print any node outputs\n",
    "        for key, value in output.items():\n",
    "            if \"messages\" in value:\n",
    "                print(value[\"messages\"][0].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test our chatbot out and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is complexity economics?\"\n",
    "ask(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
