{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Like I'm 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture](images/architecture.png)\n",
    "\n",
    "In this notebook, we're going to walk through setting up a simple chatbot in LangGraph. \n",
    "\n",
    "Throughout this process, we're going to show how LangSmith can be used to improve the developer experience for AI applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading our environment variables from our .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "# Loads the following env variables\n",
    "# LANGSMITH_TRACING=true\n",
    "# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGSMITH_PROJECT=\"eli5\"\n",
    "# LANGSMITH_API_KEY=\"<redacted>\"\n",
    "# OPENAI_API_KEY=\"<redacted>\"\n",
    "# TAVILY_API_KEY=\"<redacted>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a tool called Tavily to allow our assistant to search the web when answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a prompt for RAG that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
      "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
      "You have provided with relevant background context to answer the question.\n",
      "\n",
      "Question: {question} \n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Our Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the State for our Graph. We'll track the user's question, our application's generation, and the list of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's define the nodes of our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Web search\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "    \n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(question=question, context=\"\\n\".join([d.page_content for d in documents]))\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function to pretty print our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    for output in app.stream({\"question\": question}, stream_mode=\"updates\"):\n",
    "        if END in output or START in output:\n",
    "            continue\n",
    "        # Print any node outputs\n",
    "        for key, value in output.items():\n",
    "            if \"messages\" in value:\n",
    "                print(value[\"messages\"][0].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test our chatbot out and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is complexity economics?\"\n",
    "ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing Without Langchain or LangGraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "@traceable\n",
    "def search(question):\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    return web_results\n",
    "    \n",
    "@traceable\n",
    "def explain(question, context):\n",
    "    formatted = prompt.format(question=question, context=context)\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": formatted},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def eli5(question):\n",
    "    context = search(question)\n",
    "    answer = explain(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system_message' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat is complexity economics?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43meli5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/musically/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py:634\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    633\u001b[39m     _on_run_end(run_container, error=e)\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    635\u001b[39m _on_run_end(run_container, outputs=function_result)\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m function_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/musically/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py:631\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func_accepts_parent_run:\n\u001b[32m    630\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_tree\u001b[39m\u001b[33m\"\u001b[39m] = run_container[\u001b[33m\"\u001b[39m\u001b[33mnew_run\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m     function_result = \u001b[43mrun_container\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    633\u001b[39m     _on_run_end(run_container, error=e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36meli5\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@traceable\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meli5\u001b[39m(question):\n\u001b[32m     28\u001b[39m     context = search(question)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     answer = \u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/musically/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py:634\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    633\u001b[39m     _on_run_end(run_container, error=e)\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    635\u001b[39m _on_run_end(run_container, outputs=function_result)\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m function_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/musically/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py:631\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func_accepts_parent_run:\n\u001b[32m    630\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_tree\u001b[39m\u001b[33m\"\u001b[39m] = run_container[\u001b[33m\"\u001b[39m\u001b[33mnew_run\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m     function_result = \u001b[43mrun_container\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    633\u001b[39m     _on_run_end(run_container, error=e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mexplain\u001b[39m\u001b[34m(question, context)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@traceable\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexplain\u001b[39m(question, context):\n\u001b[32m     15\u001b[39m     formatted = prompt.format(question=question, context=context)\n\u001b[32m     17\u001b[39m     completion = openai_client.chat.completions.create(\n\u001b[32m     18\u001b[39m         messages=[\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43msystem_message\u001b[49m},\n\u001b[32m     20\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: question},\n\u001b[32m     21\u001b[39m         ],\n\u001b[32m     22\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     )\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m completion.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[31mNameError\u001b[39m: name 'system_message' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"What is complexity economics?\"\n",
    "print(eli5(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
